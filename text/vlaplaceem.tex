\documentclass[11pt]{article}
\usepackage[pdftex]{graphicx} 
\usepackage{natbib} % round/square, authoryear/numbers
\usepackage{amsfonts,amsmath,amssymb,amsthm} % various math stuff
% \usepackage[compact]{titlesec}  % Allow compact titles
\usepackage{sectsty} % Allows for section style to be manipulated
\usepackage{wrapfig}  % Allow wrapping of text around figures
\usepackage{mdwlist}  % Make list items closer together: itemize*, enumerate*
\usepackage{sidecap}  % For putting caption beside figure
\usepackage{color} % colors
\usepackage{lineno}
\usepackage[labelsep=period]{caption}
\usepackage{lscape}


% ==  FONT  ===========================
% % ---- uncomment for Palatino -------------
% \usepackage{palatino} 
% ---- uncomment for Helvetica (sansserif)  -------------
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault}  % If the base font of the document is to be sans serif
\usepackage{algorithm}
\usepackage{algpseudocode}

% ======= File locations =================
\input{jpdefs} % define some useful latex commands
\input{dzdefs} 
%\newcommand{\figdir}{figs}  % fig directory

\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\DeclareMathOperator*{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}

% ---- Control paragraph spacing and indenting  -------------------
\setlength{\textheight}{9in} % 11 - ( 1.5 + \topmargin + <bottom-margin> )
\setlength{\textwidth}{6.5in} % 8.5 - 2 * ( 1 + \oddsidemargin )
\setlength{\topmargin}{ -0.5in}  % in addition to 1.5'' standard margin
\setlength{\oddsidemargin}{0in} % in addition to 1'' standard
\setlength{\parindent}{0em}  % indentation 
\setlength{\parskip}{1ex} % paragraph gap 
\setlength{\headsep}{0.0in}
\setlength{\headheight}{0.0in}
% \raggedbottom  % allows pages to have differing text height 
\allowdisplaybreaks  % allow multi-line equations to continue on the next page

% ----- Float (figure) positioning --------------------------------------
\setcounter{topnumber}{1}  % max # floats at top of page
\setcounter{bottomnumber}{1} % max # at bottom of page
\setcounter{totalnumber}{1} % max floats on a page
\renewcommand{\topfraction}{1} % max frac for floats at top
\renewcommand{\bottomfraction}{1} % max for floats at bottom
\renewcommand{\textfraction}{0} % minimum fraction of page for text
%\setlength{\textfloatsep}{.2in}  % separation between figure and text


% -----  New definitions ------------------
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}  % use for citing figs

% ====================================================
\begin{document}
% % ----- Math spacing commands that have to be within document ----------
% \abovedisplayskip=2pt  % spacing above an equation
% \belowdisplayskip=2pt  % spacing below an equation
% %\abovedisplayshortskip=-5pt
% % \belowdisplayshortskip=-5pt
% % --------------------------------------------------------------


% % --- Publication note ---------
%\vbox to 0.2in{ \vspace{-.3in}
% \small\tt  Draft Information
% \vfil}

% Title
\title{Variational Laplace EM for inference in recurrent switching state-space models} 

% \author{David M. Zoltowski \& Jonathan W. Pillow \\ 
%   Princeton Neuroscience Institute \\
%   Princeton University\\
%   \texttt{zoltowski@princeton.edu, pillow@princeton.edu}} \\
% \author{Scott W. Linderman \\ 
%   ?? \\
%   Stanford University\\
%   \texttt{?}}

\maketitle


\setspacing{1.1}
\hbox to \textwidth{\hrulefill} 
\vspace{-.1in}

% Abstract
{\bf We develop variational Laplace-EM for inference in recurrent switching state-space models.}\\
\hbox to \textwidth{\hrulefill}

\section{Recurrent switching state-space models}

We model $N$-dimensional observations $y_t$ at each time $t \in \{ 1, 2, \cdots, T\}$ as emissions from a recurrent switching state-space model, or recurrent switching linear dynamical system (rSLDS). The models have a $D$-dimensional continuous latent state $x_t \in \mathbb{R}^D$, a set of $K$ discrete latent states $z_t \in \{ 1, 2, \cdot, K \}$, and $M$-dimensional inputs $u_t \in \mathbb{R}^M$. 

We define the rSLDS following the treatment in \cite{linderman2017bayesian}.

The continuous latent state $x_t$ follows linear-Gaussian dynamics that depend on the discrete state $z_t$
\be
x_{t} = A_{z_t} x_{t-1} + V_{z_t} u_t + b_{z_t} + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, Q_{z_t})
\ee
where $A_{z_t}, Q_{z_t} \in \mathbb{R}^{D \times D}$, $V_{z_t} \in \mathbb{R}^{D \times M}$, and $b_{z_t} \in \mathbb{R}^D$ for each $z_t$. 

In general, in an rSLDS the discrete state at time $t+1$ depends on the discrete and continuous states at time $t$ and the current input at time $t+1$. The transition probabilities are given by \textbf{(note right hand side is a vector)}
\be
\log p(z_{t+1} | z_t, x_t, u_{t+1}, R_{z_t}, W_{z_t}, r_{z_t}) \propto R_{z_t} x_t + r_{z_t} + W_{z_t} u_{t+1}
\ee
where the dependence on the continuous state is controlled by $R_{z_t} \in \mathbb{R}^{K \times D}$, the dependence on the input is controlled by $W_{z_t} \in \mathbb{R}^{K \times M}$, and $r_{z_t} \in \mathbb{R}^M$ is a bias that captures dependencies in the discrete states. 
%In an rSLDS, the observations are linear mappings from the continuous latent states 
%\be
%y_t = C_{z_t} x_t + d_{z_t} + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, S_{z_t}). 
%\ee
We consider general emissions mappings from the latent variables to the observations of the form \textbf{TODO: include other emissions parameters?}
\be
y_t \sim \mathcal{P}( f( C_{z_t} x_t + d_{z_t} ) ). 
\ee

\section{Variational Laplace-EM for Inference}

Related work: 
\begin{enumerate}
\item Inference in recurrent switching state-space models \cite{linderman2017bayesian,nassar2018treestructured,linderman2019hierarchical}
\item aSLDS \cite{barber2006expectation, barberBRML2012}
\item Variational Laplace-EM \cite{wang2013variational}
\item inference in Poisson LDS or state-space models models \cite{macke2011empirical,paninski2010new, macke2015estimating, archer2015black}
\item Turner, Sahani \cite{turner2011two}
\end{enumerate}

Here we develop a variational Laplace-EM method for performing inference in recurrent switching state-space models. We introduce approximate posterior distributions $q(z)$ and $q(x)$ over the discrete and continuous latent variables, respectively. With those distributions we have a lower-bound on the marginal likelihood 
\begin{align*}
\mathcal{L}_q(\theta) & = \mathbb{E}_{q(z) q(x)}[\log p(x, z, y | \theta) - \log q(z) q(x)] \\
& = \mathbb{E}_{q(z) q(x)}[\log p(x, z, y | \theta)] - \mathbb{E}_{q(z)}[\log q(z)] - \mathbb{E}_{q(x)}[\log q(x)].
\end{align*}
To optimize this objective, we alternate between updating 1) $q(z)$, 2) $q(x)$ and 3) $\theta$. 

%%%%
% q(z)
%%%%
\subsection{Update $q(z)$}

We update $q(z)$ via the optimal coordinate ascent variational inference update
\be
q^\star(z)  \propto \exp ( \mathbb{E}_{q(x)}[\log p(x, z, y | \theta)] ).
\ee
We expand the expected joint log joint probability
\begin{align*}
\mathbb{E}_{q(x)}[\log p(x, z, y | \theta)]  & = \mathbb{E}_{q(x)} [ \log p(z_1, x_1 | \theta) + \sum_{t=2}^{T} \log p(x_t | x_{t-1}, z_t, \theta) + \sum_{t=1}^{T-1} \log p(z_{t+1} | z_t, x_t, \theta) + \sum_{t=1}^T \log p(y_t | x_t, z_t, \theta)] \\
& = \phi(z_1, x_1) + \sum_{t=2}^{T} \phi(z_t, x_t, x_{t-1}) + \sum_{t=1}^{T-1} \phi(z_t, z_{t+1}, x_t) + \sum_{t=1}^T \phi(z_t, x_t, y_t)
\end{align*}
where we have introduced the potentials 
\begin{align*}
\phi(z_1, x_1) & =  \mathbb{E}_{q(x)}[\log p(z_1, x_1 | \theta)] \\
\phi(z_t, x_t, x_{t-1}) & = \mathbb{E}_{q(x)} [\log p(x_t | x_{t-1}, z_t, \theta)] \\
\phi(z_t, z_{t+1}, x_t) & =  \mathbb{E}_{q(x)} [\log p(z_{t+1} | z_t, x_t, \theta) ] \\
\phi(z_t, x_t, y_t) & = \mathbb{E}_{q(x)} [\log p(y_t | x_t, z_t, \theta)].
\end{align*}
The update to $q^\star(z)$ is computed by drawing samples from $q(x)$. Conditioned on these samples, we have an HMM and the necessary marginals over $z$ (the unity and pairwise marginals) can be computed via message passing algorithms. We note that if the emissions are independent of the discrete states when conditioned on the continuous states
\be
\log p(y_t | x_t, z_t, \theta) = \log p(y_t | x_t, \theta)
\ee
then the emissions potential $\phi(z_t, x_t, y_t)$ can be disregarded when conditioning on a sample from $q(x)$. 


We introduce the normalizing constant of the distribution $Z(\theta)$ such that
\be
q(z) = \frac{1}{Z(\theta)} \exp \bigg(  \phi(z_1, x_1) + \sum_{t=2}^{T} \phi(z_t, x_t, x_{t-1}) + \sum_{t=1}^{T-1} \phi(z_t, z_{t+1}, x_t) + \sum_{t=1}^T \phi(z_t, x_t, y_t) \bigg)
\ee
and
\be
\log q(z) = - \log Z(\theta) + \phi(z_1, x_1) + \sum_{t=2}^{T} \phi(z_t, x_t, x_{t-1}) + \sum_{t=1}^{T-1} \phi(z_t, z_{t+1}, x_t) + \sum_{t=1}^T \phi(z_t, x_t, y_t).
\ee
We can evaluate the entropy term in the ELBO 
\begin{align*}
\mathbb{E}_{q(z)}[\log q(z)] & = \mathbb{E}_{q(z)}[- \log Z(\theta) + \phi(z_1, x_1) + \sum_{t=2}^{T} \phi(z_t, x_t, x_{t-1}) + \sum_{t=1}^{T-1} \phi(z_t, z_{t+1}, x_t) + \sum_{t=1}^T \phi(z_t, x_t, y_t)] \\
& = -\log Z(\theta) + \mathbb{E}_{q(z)} [\phi(z_1, x_1)] + \sum_{t=2}^{T} \mathbb{E}_{q(z)}  [ \phi(z_t, x_t, x_{t-1}) ] + \sum_{t=1}^{T-1} \mathbb{E}_{q(z)} [\phi(z_t, z_{t+1}, x_t)] \\
& \quad + \sum_{t=1}^T  \mathbb{E}_{q(z)} [\phi(z_t, x_t, y_t)]
\end{align*}
using the potentials, the unitary and pairwise marginals, and the normalizing constant.

\subsection{Update $q(x)$}
The update $q^\star(x)$ is given by a Laplace approximation around the mode of $\mathbb{E}_{q(z)}[\log p(x, z, y | \theta)]$ such that 
\be
q^\star(x)  = \mathcal{N}(x^\star, -H^{-1}) 
\ee
where 
\begin{align*}
x^\star &= \argmax_x \mathbb{E}_{q(z)}[\log p(x, z, y | \theta)] \\
H &= \nabla_x^2 \mathbb{E}_{q(z)}[\log p(x, z, y | \theta)] \bigg|_{x = x^\star}.
\end{align*}
We expand the terms in the objective
\begin{align*}
\mathcal{L}(x) &= \mathbb{E}_{q(z)}[\log p(x, z, y | \theta)] \\
& = \mathbb{E}_{q(z)} [ \log p(z_1 | \theta) +  \log p(x_1 | z_1, \theta) + \sum_{t=2}^{T} \log p(x_t | x_{t-1}, z_t, \theta) + \sum_{t=1}^{T-1} \log p(z_{t+1} | z_t, x_t, \theta) + \sum_{t=1}^T \log p(y_t | x_t, z_t, \theta)] \\
& = \phi(x_1, z_1) + \sum_{t=2}^T \phi(x_t, x_{t-1}, z_t) + \sum_{t=1}^{T-1} \phi(x_t, z_t, z_{t+1}) + \sum_{t=1}^T \phi(x_t, y_t, z_t) + \text{const} 
\end{align*}
where
\begin{align*}
\phi(x_1, z_1) & =  \mathbb{E}_{q(z)}[\log p(x_1 |  z_1 , \theta)] = \sum_k q(z_1 = k) \log p(x_1 | z_1 = k, \theta) \\
\phi(x_t, x_{t-1}, z_t) & = \mathbb{E}_{q(z)}[ \log p(x_t | x_{t-1}, z_t, \theta) ] = \sum_{k} q(z_t = k) \log p(x_t | x_{t-1}, z_t = k, \theta)  \\
\phi(x_t, z_t, z_{t+1}) & = \mathbb{E}_{q(z)}[  \log p(z_{t+1} | z_t, x_t, \theta) ] = \sum_k \sum_j q(z_t = k, z_{t+1} = j) \log p(z_{t+1} = j | z_t = k, x_t, \theta) \\  
\phi(x_t, y_t, z_t) & = \mathbb{E}_{q(z)}[  \log p(y_t | x_t, z_t, \theta) ] = \sum_k q(z_t = k) \log p(y_t | x_t , z_t = k, \theta) .
\end{align*}

The above derivation was written in full generality. If the emissions do not depend on the discrete state the emissions potentials simplify to 
\be
\phi(x_t, y_t, z_t) =  \log p(y_t | x_t, \theta).
\ee
Also, importantly, if there is no recurrent dependencies (e.g. in an SLDS) then the transition term  $\log p(z_{t+1} | z_t, x_t, \theta) = \log p(z_{t+1} | z_t, \theta)$ and the transitions potential $\phi(x_t, z_t, z_{t+1})$ drops out as it no longer depends on $x_t$. 


We require the Hessian matrix for the Laplace approximation. This matrix is given by
\begin{align*}
\nabla_x^2 \mathcal{L}(x) & = \nabla_x^2 \mathbb{E}_{q(z)}[\log p(x, z, y | \theta)] \\
& = \nabla_x^2 \phi(x_1, z_1) + \sum_{t=2}^T \nabla_x^2 \phi(x_t, x_{t-1}, z_t) + \sum_{t=1}^{T-1} \nabla_x^2 \phi(x_t, z_t, z_{t+1}) + \sum_{t=1}^T \nabla_x^2 \phi(x_t, y_t, z_t) 
\end{align*}
where
\begin{align*}
\nabla_x^2 \phi(x_1, z_1) & = \sum_k q(z_1 = k) \nabla_x^2 \log p(x_1 | z_1 = k, \theta) \\
\nabla_x^2 \phi(x_t, x_{t-1}, z_t)  & = \sum_{k} q(z_t = k) \nabla_x^2  \log p(x_t | x_{t-1}, z_t = k, \theta)  \\
\nabla_x^2 \phi(x_t, z_t, z_{t+1}) & = \sum_k \sum_j q(z_t = k, z_{t+1} = j) \nabla_x^2 \log p(z_{t+1} = j | z_t = k, x_t, \theta) \\
\nabla_x^2 \phi(x_t, y_t, z_t) & = \sum_k q(z_t = k) \nabla_x^2 \log p(y_t | x_t , z_t = k, \theta).
\end{align*}
Therefore, we can compute the Hessian by computing the contributions to the Hessian of the dynamics, emissions, and transitions potentials. 

The Hessian $H$ has size $TD \times TD$ but has a sparse, block tridiagonal structure with blocks of size $D \times D$. The Hessians from the initial states, transitions, and emissions potentials only contribute terms to the primary block diagonal. The dynamics potentials contribute terms to both the primary and 1st off diagonal blocks. 

Throughout, we only represent the primary diagonal blocks and lower diagonal blocks of the Hessian. This reduces storage from the full $(TD)^2$ to $(2T-1)D^2$. For linear solves and matrix inversions of the Hessian, we also use algorithms that exploit the block tridiagonal structure. 

How to find $x^\star$? We have access to the Hessian matrix for the Laplace approximation. We can therefore use Newton's method with a backtracking line search to find the maximum of the objective. However, we can also use optimization routines that require only gradient information (lBFGS) or require only gradient information and Hessian-vector products (Newton-CG or trust-region Newton-CG). 

Main idea: Important Hessian terms are recurrent only (softmax) and stationary (SLDS, zeros) for transitions, Gaussian for dynamics (weighted by probability of each state), and then the emissions Hessian. 

\subsection{Update $\theta$}
We update the model parameters by approximately optimizing the lower bound
\be
\theta^\star = \argmax_\theta  \mathbb{E}_{q(z) q(x)}[\log p(x, z, y | \theta) - \log q(z) q(x)]. 
\ee 
In the update of $\theta$, instead of optimizing the expectation under the full distribution of $q(x)$ we optimize
\be
\theta^\star = \argmax_\theta \mathbb{E}_{q(z)}[\log p(\hat{x}, z, y | \theta)]
\ee
where $\hat{x}$ is a sample from $q(x)$ and we have dropped terms that do not depend on $\theta$. Conditioned on $\hat{x}$, the update consists of M-steps on the transitions, dynamics, and emissions parameters. We use either exact updates (where applicable) or lFBGS to implement the M-steps. Finally, we set the parameters at iteration $i$ via a convex combination of the new parameters $\theta^\star$ and the parameters at the previous iteration
\be
\theta_i = (1 - \alpha) \, \theta^\star + \alpha \, \theta_{i-1}.
\ee

\subsection{Hessian terms}

The emissions terms contributing to the $t$-th diagonal block of the Hessian for $t = (1, \cdots, T)$ are 
\be
\mathbb{E}_{q(z_t)}[ \nabla_{x_t}^2 \log p(y_t | x_t, z_t, \theta)].
\ee
The transition terms contributing to the $t$-th diagonal block of the Hessian for $t = (1, \cdots, T-1)$ are
\be
\mathbb{E}_{q(z_t, z_{t+1})}[ \nabla_{x_t}^2 \log p(z_{t+1} | x_t, z_t, \theta)].
\ee
The contribution of the transition terms to the $T$-th diagonal block is zero. 

As described above, here we work with linear Gaussian dynamics for the continuous latents such that
\begin{align*}
\mathbb{E}_{q(z)}[\log p(x, z | \theta)] & = \mathbb{E}_{q(z)}[ \log p(z_1 | \theta) + \log p(x_1 | z_1, \theta) + \sum_{t=2}^T \log p(x_t | x_{t-1}, z_t, \theta)] \\
& = \mathbb{E}_{q(z)}[ -\frac{1}{2} (x_1 - \mu_{0,z_1})^\top Q_{0,z_1}^{-1} (x_1 - \mu_{0,z_1}) \\
& \quad \quad \quad \, \, \, -\frac{1}{2} \sum_{t=2}^T (x_t - A_{z_t} x_{t-1})^\top Q_{z_t}^{-1} (x_t - A_{z_t} x_{t-1})] + \text{const}.
\end{align*}
Note, we have ignored the offset $b_{z_t}$ and input-dependence $V_{z_t}$ here because they do not contribute to the Hessian. The form of the Hessian follows from \cite{macke2015estimating}. 

Dynamics term:
\be
L = \mathbb{E}_{q(z)}[ -\frac{1}{2} \sum_{t=2}^T ( x_t^\top Q_{z_t}^{-1} x_t - 2 x_t^\top Q_{z_t}^{-1} A_{z_t} x_{t-1} + x_{t-1}^\top A_{z_t}^\top Q_{z_t}^{-1} A_{z_t} x_{t-1}) ]
\ee
For the diagonal blocks from $t = (2, \cdots, T)$
\be
\nabla_{x_t}^2 L = -\frac{1}{2} \mathbb{E}_{q(z_t)}[\nabla_{x_t}^2 x_t^\top Q_{z_t}^{-1} x_t ] = \mathbb{E}_{q(z_t)}[ - Q_{z_t}^{-1} ].
\ee
This term contributes to the diagonal blocks from $t = (1, \cdots, T-1)$, where the expectations over the discrete state are taken at time $t+1$
\be
\nabla_{x_{t-1}}^2 L = -\frac{1}{2} \mathbb{E}_{q(z_t)}[\nabla_{x_{t-1}}^2 x_{t-1}^\top A_{z_t}^\top Q_{z_t}^{-1} A_{z_t} x_{t-1}) ] = \mathbb{E}_{q(z_t)}[ - A_{z_t}^\top Q_{z_t}^{-1} A_{z_t} ].
\ee
The contributes to the lower-diagonal blocks. 
\be
\nabla_{x_t} \nabla_{x_{t-1}} L = \mathbb{E}_{q(z_t)}[\nabla_{x_t} \nabla_{x_{t-1}} x_t^\top Q_{z_t}^{-1} A_{z_t} x_{t-1}] = \mathbb{E}_{q(z_t)}[\nabla_{x_t} A_{z_t}^\top Q_{z_t}^{-1} x_t] = \mathbb{E}_{q(z_t)}[Q_{z_t}^{-1} A_{z_t}]
\ee

{\small
\be
\begin{pmatrix}
-\mathbb{E}_{z_1}[Q_{0,z_t}^{-1}] - \mathbb{E}_{q(z_2)}[A_{z_t}^\top Q_{z_t}^{-1} A_{z_t}] & \mathbb{E}_{q(z_2)}[A_{z_t}^\top Q_{z_t}^{-1} ] & \cdots & 0 \\
\mathbb{E}_{q(z_2)}[Q_{z_t}^{-1} A_{z_t}] & -\mathbb{E}_{q(z_2)}[Q_{z_t}^{-1}] - \mathbb{E}_{q(z_3)}[A_{z_t}^\top Q_{z_t}^{-1} A_{z_t}] & \mathbb{E}_{q(z_3)}[A_{z_t}^\top Q_{z_t}^{-1} ] & \vdots \\
\vdots & \mathbb{E}_{q(z_3)}[Q_{z_t}^{-1} A_{z_t}] & -\mathbb{E}_{q(z_3)}[Q_{z_t}^{-1}] - \mathbb{E}_{q(z_4)}[A_{z_t}^\top Q_{z_t}^{-1} A_{z_t}] & \mathbb{E}_{q(z_4)}[A_{z_t}^\top Q_{z_t}^{-1} ] \\
0 & \cdots & \mathbb{E}_{q(z_T)}[Q_{z_t}^{-1} A_{z_t}] & -\mathbb{E}_{q(z_T)}[Q_{z_t}^{-1}]
\end{pmatrix}
\ee
}

\begin{itemize}
\item Example option 1: SLDS with Poisson exp emissions. Gaussian dynamics. Other terms drop out.
\item Example option 2: rSLDS with Poisson exp emissions, Gaussian dynamics, recurrent only transitions. Requires recurrent only Hessian.
\item Hessian terms for Gaussian dynamics; recurrent only and stationary transitions; and Poisson exponential and softplus, bernoulli, and Gaussian emissions.
\end{itemize}


\small
\bibliographystyle{plain}
%\bibliographystyle{apalike}
\bibliography{dzbib.bib}

\end{document}

