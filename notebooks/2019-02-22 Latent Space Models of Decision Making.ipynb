{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for initialization\n",
    "from ssm.optimizers import adam_step, rmsprop_step, sgd_step, convex_combination\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.tracer import getval\n",
    "from autograd.misc import flatten\n",
    "from autograd import value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ssm.models import HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Drift-diffusion models are common models of choice behavior and neural activity in 2AFC decision tasks (Gold and Shadlen, *Annu. Rev. Neurosci.* 2007; Ratcliff, *Neural Computation* 2008), where they are typically fit to the set of choices and reaction times in an experiment. Recent work has fit neural instantiations of the DDM to spiking responses (Latimer et al., *Science* 2015; Zoltowski et al., *bioRxiv* 2018). While many formulations of DDM-like models exist in the literature, here we aim to describe how such models can be instantiated as state-space models. We can then leverage the flexibility of this framework to generalize the models in interesting ways and fit them to neural activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a simple drift-diffusion model (DDM)  \n",
    "\n",
    "The state is a one dimensional variable $x_t \\in \\mathbb{R}$. There is a discrete state $z_t \\in \\{\\mathsf{ramp}, \\mathsf{left bound}, \\mathsf{right bound}\\}$ that indicates whether the continuous state has reached its threshold (bound). Let $u_t \\in \\mathbb{R}$ be the input, which specifiy the amount of evidence to the left or right choice, which governs the dynamics of $x_t$. \n",
    "\n",
    "Model:\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_t &\\sim p(x_{t-1}) \\\\\n",
    "x_t &= x_{t-1} + V_{z_t} u_t + \\epsilon_t \n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ and the input weights are constrained such that\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_{\\mathsf{ramp}} &= \\beta, \\\\\n",
    "V_{\\mathsf{left bound}} = V_{\\mathsf{right bound}} &= 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "The discrete transition probabilities are,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(z_t = \\textsf{left bound} \\mid x_{t-1}) &\\propto x_{t-1} - 1 \\\\\n",
    "\\log p(z_t = \\textsf{ramp} \\mid x_{t-1}) &\\propto 0 \\\\\n",
    "\\log p(z_t = \\textsf{right bound} \\mid x_{t-1}) &\\propto -x_{t-1} - 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmm = HMM(K=3, D=1, M=1, observations=\"diagonal_ar\", transitions=\"recurrent_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.init_state_distn.log_pi0 = np.log([0.01, 0.98, 0.01])\n",
    "\n",
    "# All the states have dynamics matrix of 1 (identity) and bias of 0\n",
    "hmm.observations.As = np.ones((3, 1, 1))\n",
    "hmm.observations.bs = np.zeros((3, 1))\n",
    "hmm.observations._log_sigmasq = np.log(np.array([1e-5, 1e-3, 1e-5])).reshape((3, 1))\n",
    "hmm.observations.mu_init = np.zeros((3, 1))\n",
    "hmm.observations._log_sigmasq_init = np.log(.01 * np.ones((3, 1)))\n",
    "\n",
    "# They only differ in their input \n",
    "beta = 1\n",
    "hmm.observations.Vs[0] = 0       # left bound\n",
    "hmm.observations.Vs[1] = beta    # ramp\n",
    "hmm.observations.Vs[2] = 0       # right bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 100\n",
    "hmm.transitions.Ws = np.zeros((3, 1))\n",
    "hmm.transitions.Rs = np.array([w, 0, -w]).reshape((3, 1))\n",
    "hmm.transitions.r = np.array([-w, 0, -w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample state trajectories\n",
    "T = 100\n",
    "u = .015 * np.ones((T, 1))\n",
    "\n",
    "samples = [hmm.sample(T, input=u) for _ in range(100)]\n",
    "zs, xs = zip(*samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "plt.plot(1 - zs[i])\n",
    "plt.plot(xs[i][:, 0])\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(xs, axis=0)[:, 0]\n",
    "x_std = np.std(xs, axis=0)[:, 0]\n",
    "\n",
    "plt.fill_between(np.arange(T), x_mean-2*x_std, x_mean+2*x_std, color='k', edgecolor=\"none\", alpha=0.2)\n",
    "plt.plot(x_mean, '-k', lw=3)\n",
    "for x in xs:\n",
    "    plt.plot(x, '-k', lw=.5, alpha=.25)\n",
    "\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"$x(t)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm.core import BaseHMM\n",
    "from ssm.init_state_distns import InitialStateDistribution\n",
    "from ssm.transitions import _Transitions, RecurrentOnlyTransitions\n",
    "from ssm.observations import _Observations, AutoRegressiveDiagonalNoiseObservations\n",
    "\n",
    "class DDMObservations(AutoRegressiveDiagonalNoiseObservations):\n",
    "    def __init__(self, K, D, M=1, lags=1, beta=1.0, sigmas=np.array([[1e-5], [1e-3], [1e-5]])):\n",
    "        assert K == 3\n",
    "        assert D == 1\n",
    "        assert M == 1\n",
    "        super(DDMObservations, self).__init__(K, D, M)\n",
    "        \n",
    "        # The only free parameters of the DDM are the ramp rate...\n",
    "        self.beta = beta\n",
    "        \n",
    "        # and the noise variances, which are initialized in the AR constructor\n",
    "        self._log_sigmasq = np.log(sigmas)\n",
    "        \n",
    "        # Set the remaining parameters to fixed values\n",
    "        self._As = np.ones((3, 1, 1))\n",
    "        self.bs = np.zeros((3, 1))\n",
    "        self.mu_init = np.zeros((3, 1))\n",
    "        self._log_sigmasq_init = np.log(.01 * np.ones((3, 1)))\n",
    "\n",
    "        # They only differ in their input \n",
    "        self.Vs[0] = 0            # left bound\n",
    "        self.Vs[1] = self.beta    # ramp\n",
    "        self.Vs[2] = 0            # right bound\n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self._log_sigmasq\n",
    "        \n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        self.beta, self._log_sigmasq = value\n",
    "        mask = np.reshape(np.array([0, 1, 0]), (3, 1, 1))\n",
    "        self.Vs = mask * self.beta\n",
    "        \n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None):\n",
    "        pass\n",
    "\n",
    "    def m_step(self, expectations, datas, inputs, masks, tags, **kwargs):\n",
    "        _Observations.m_step(self, expectations, datas, inputs, masks, tags, **kwargs)\n",
    "        \n",
    "\n",
    "# Do the same for the transition model\n",
    "class DDMTransitions(RecurrentOnlyTransitions):\n",
    "    def __init__(self, K, D, M=0, scale=100):\n",
    "        assert K == 3\n",
    "        assert D == 1\n",
    "        assert M == 1\n",
    "        super(DDMTransitions, self).__init__(K, D, M)\n",
    "\n",
    "        # Parameters linking past observations to state distribution\n",
    "        self.Ws = np.zeros((3, 1))\n",
    "        self.Rs = np.array([scale, 0, -scale]).reshape((3, 1))\n",
    "        self.r = np.array([-scale, 0, -scale])\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return ()\n",
    "    \n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None):\n",
    "        pass\n",
    "    \n",
    "    def m_step(self, expectations, datas, inputs, masks, tags, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "def DDM(beta=1.0, sigmas=np.array([[1e-3], [1e-3], [1e-3]])):\n",
    "    K, D, M = 3, 1, 1\n",
    "    \n",
    "    # Build the initial state distribution, the transitions, and the observations\n",
    "    init_state_distn = InitialStateDistribution(K, D, M)\n",
    "    init_state_distn.log_pi0 = np.log([0.01, 0.98, 0.01])\n",
    "    transition_distn = DDMTransitions(K, D, M)\n",
    "    observation_distn = DDMObservations(K, D, M, beta=beta, sigmas=sigmas)\n",
    "    \n",
    "    return BaseHMM(K, D, M, init_state_distn, transition_distn, observation_distn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([[1e-4],[1e-3],[1e-4]])\n",
    "ddm = DDM(sigmas=sigmas)\n",
    "\n",
    "# Sample state trajectories\n",
    "T = 100\n",
    "N_samples = 100\n",
    "\n",
    "us = []\n",
    "zs = []\n",
    "xs = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "    u = -.025 + .05 * np.random.rand() * np.ones((T, 1))\n",
    "    z, x = ddm.sample(T, input=u)\n",
    "    \n",
    "    us.append(u)\n",
    "    zs.append(z)\n",
    "    xs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(xs, axis=0)[:, 0]\n",
    "x_std = np.std(xs, axis=0)[:, 0]\n",
    "\n",
    "plt.fill_between(np.arange(T), x_mean-2*x_std, x_mean+2*x_std, color='k', edgecolor=\"none\", alpha=0.2)\n",
    "plt.plot(x_mean, '-k', lw=3)\n",
    "for x in xs:\n",
    "    plt.plot(x, '-k', lw=.5, alpha=.25)\n",
    "\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"$x(t)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ddm = DDM(beta=0.0, sigmas=np.ones((3, 1)))\n",
    "# test_ddm = DDM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = test_ddm.fit(xs, inputs=us, num_em_iters=25)\n",
    "\n",
    "plt.plot(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddm.observations.beta)\n",
    "print(ddm.observations.sigmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ddm.observations.beta)\n",
    "print(test_ddm.observations.sigmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True: \", ddm.log_likelihood(xs, inputs=us))\n",
    "print(\"Test: \", test_ddm.log_likelihood(xs, inputs=us))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a latent version of the DDM with linear emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ssm.preprocessing import interpolate_data, pca_with_imputation\n",
    "from ssm.preprocessing import factor_analysis_with_imputation\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm.emissions import _LinearEmissions, GaussianEmissions, PoissonEmissions\n",
    "from ssm.core import BaseSwitchingLDS\n",
    "from ssm.util import ensure_args_are_lists\n",
    "\n",
    "\n",
    "class DDMGaussianEmissions(GaussianEmissions):\n",
    "    def __init__(self, N, K, D, M=0, single_subspace=True):\n",
    "        super(DDMGaussianEmissions, self).__init__(N, K, D, M=M, single_subspace=single_subspace)\n",
    "        # Make sure the input matrix Fs is set to zero and never updated\n",
    "        self.Fs *= 0\n",
    "\n",
    "    # Construct an emissions model\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._Cs, self.ds, self.inv_etas\n",
    "\n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        self._Cs, self.ds, self.inv_etas = value\n",
    "\n",
    "    @ensure_args_are_lists\n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None, num_em_iters=50, num_tr_iters=50):\n",
    "        \n",
    "        print(\"Initializing...\")\n",
    "        print(\"First with FA using {} steps of EM.\".format(num_em_iters))\n",
    "        fa, xhats, Cov_xhats, lls = factor_analysis_with_imputation(self.D, datas, masks=masks, num_iters=num_em_iters)\n",
    "\n",
    "        # define objective\n",
    "        def _objective(params, itr):\n",
    "            Td = sum([x.shape[0] for x in xhats])\n",
    "            new_datas = [np.dot(x,params[0].T)+params[1] for x in xhats]\n",
    "            obj = DDM().log_likelihood(new_datas,inputs=inputs)\n",
    "            return -obj / Td\n",
    "        \n",
    "        # initializeR and r\n",
    "        R = 0.1*np.random.randn(self.D,self.D)\n",
    "        r = 0.1*np.random.randn(self.D)\n",
    "        params = [R,r]\n",
    "        Td = sum([x.shape[0] for x in xhats])\n",
    "        \n",
    "        print(\"Next by transforming latents to match DDM prior using {} steps of max log likelihood.\".format(num_tr_iters))\n",
    "\n",
    "        state = None\n",
    "        lls = [-_objective(params, 0) * Td]\n",
    "        pbar = trange(num_tr_iters)\n",
    "        pbar.set_description(\"Epoch {} Itr {} LP: {:.1f}\".format(0, 0, lls[-1]))\n",
    "\n",
    "        for itr in pbar:\n",
    "            params, val, g, state = sgd_step(value_and_grad(_objective), params, itr, state)\n",
    "            lls.append(-val * Td)\n",
    "            pbar.set_description(\"LP: {:.1f}\".format(lls[-1]))\n",
    "            pbar.update(1)\n",
    "    \n",
    "        R = params[0]\n",
    "        r = params[1]\n",
    "        \n",
    "        self.Cs = (fa.W @ np.linalg.inv(R)).reshape([self.D,self.N,self.D])\n",
    "        self.ds = fa.mean - fa.W @ np.linalg.inv(R) @ r\n",
    "        self.inv_etas[:,...] = np.log(fa.sigmasq) \n",
    "        \n",
    "#     def invert(self, data, input=None, mask=None, tag=None):\n",
    "#         return self._invert(data, input=input, mask=mask, tag=tag)\n",
    "    \n",
    "#     def _invert(self, data, input=None, mask=None, tag=None):\n",
    "        # this should return xhat! e.g. for variational inference. use initialization procedure here... \n",
    "#         pass\n",
    "        \n",
    "def LatentDDM(N, beta=1.0, sigmas=np.array([[1e-5], [1e-3], [1e-5]])):\n",
    "    K, D, M = 3, 1, 1\n",
    "    \n",
    "    # Build the initial state distribution, the transitions, and the observations\n",
    "    init_state_distn = InitialStateDistribution(K, D, M)\n",
    "    init_state_distn.log_pi0 = np.log([0.01, 0.98, 0.01])\n",
    "    transition_distn = DDMTransitions(K, D, M)\n",
    "    dynamics_distn = DDMObservations(K, D, M, beta=beta, sigmas=sigmas)\n",
    "\n",
    "    emission_distn = DDMGaussianEmissions(N, K, D, M=M, single_subspace=True)\n",
    "    \n",
    "#     @ensure_args_are_lists\n",
    "#     def log_probability(self, datas, inputs=None, masks=None, tags=None):\n",
    "#         return self.log_likelihood(datas, inputs, masks, tags) + self.log_prior()\n",
    "\n",
    "    # Make the SLDS\n",
    "    return BaseSwitchingLDS(N, K, D, M, init_state_distn, transition_distn, dynamics_distn, emission_distn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the latents to \"neural\" observations\n",
    "N = 10\n",
    "C = np.random.rand(N, 1)\n",
    "# C, _, _ = np.linalg.svd(C, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [np.dot(x, C.T) +1.0 + .01 * np.random.randn(T, N) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_ddm = LatentDDM(N)\n",
    "latent_ddm.emissions.initialize(ys, inputs=us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(latent_ddm.emissions)\n",
    "# print(latent_ddm.emissions._Cs)\n",
    "print(latent_ddm.emissions.Cs)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variational posterior\n",
    "from ssm.variational import SLDSMeanFieldVariationalPosterior\n",
    "q_mf = SLDSMeanFieldVariationalPosterior(latent_ddm, ys, inputs=us, initial_variance=0.01)\n",
    "# q_mf._params = [(np.random.randn(T, 1), np.zeros((T, 1))) for _ in range(N_samples)]\n",
    "# q_mf._params = [(xs[i], np.log(.01) * np.ones((T, 1))) for i in range(N_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mf_elbos = latent_ddm.fit(q_mf, ys, inputs=us, method=\"svi\", num_iters=100, initialize=False, step_size=0.1)\n",
    "\n",
    "# Get the posterior mean of the continuous states\n",
    "q_mf_x = q_mf.mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q_mf_elbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xhat in q_mf.mean:\n",
    "    plt.plot(xhat, '-k', lw=1, alpha=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 1\n",
    "tr += 1\n",
    "yhat = latent_ddm.smooth(q_mf.mean[tr], ys[tr], input=us[tr])\n",
    "plt.figure(figsize=[12,4])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(xs[tr],'k')\n",
    "plt.plot(q_mf.mean[tr],'r--')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ys[tr])\n",
    "plt.plot(yhat, '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_ddm.emissions.Cs = C.reshape([1,10,1])\n",
    "# latent_ddm.emissions.ds = np.zeros([10])\n",
    "print(latent_ddm.emissions.Cs)\n",
    "print(latent_ddm.emissions.ds)\n",
    "print(latent_ddm.emissions.inv_etas)\n",
    "# print(latent_ddm.emissions.Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddm.observations.beta)\n",
    "print(ddm.observations._log_sigmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_ddm.dynamics.beta)\n",
    "print(latent_ddm.dynamics._log_sigmasq)\n",
    "latent_ddm.dynamics.beta = ddm.observations.beta\n",
    "latent_ddm.dynamics._log_sigmasq = ddm.observations._log_sigmasq\n",
    "# print(latent_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "The latent model didn't work out of the box -- the initial state trajectories are centered to be mean zero, which isn't a reasonable assumption for this model.  \n",
    "\n",
    "Next up, let's try elaborations of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-varying input\n",
    "\n",
    "Here, the inputs are sequences of counts including zeros. This is intended to simulate the \"Poisson clicks\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_clicks(T=1.0,dt=0.01,rate_r=20,rate_l=20):\n",
    "    \"\"\"\n",
    "    This function generates right and left 'clicks' from two Poisson processes with rates rate_r and rate_l\n",
    "    over T seconds with bin sizes dt. The outputs are binned clicks into discrete time bins.\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of clicks\n",
    "    num_r = np.random.poisson(rate_r*T)\n",
    "    num_l = np.random.poisson(rate_l*T)\n",
    "    \n",
    "    # click times\n",
    "    click_time_r = np.sort(np.random.uniform(low=0.0,high=T,size=[num_r,1]))\n",
    "    click_time_l = np.sort(np.random.uniform(low=0.0,high=T,size=[num_l,1]))\n",
    "    \n",
    "    # binned outputs are arrays with dimensions Tx1\n",
    "    binned_r = np.histogram(click_time_r,np.arange(0.0,T+dt,dt))[0]\n",
    "    binned_l = np.histogram(click_time_l,np.arange(0.0,T+dt,dt))[0]\n",
    "\n",
    "    return binned_r, binned_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddm = DDM(sigmas=np.array([[1e-4], [5e-4], [1e-4]]))\n",
    "\n",
    "# Sample state trajectories\n",
    "T = 100 # number of time bins\n",
    "trial_time = 1.0 # trial length in seconds\n",
    "dt = 0.01 # bin size in seconds\n",
    "N_samples = 100 \n",
    "\n",
    "# input statistics\n",
    "total_rate = 40 # the sum of the right and left poisson process rates is 40\n",
    "\n",
    "us = []\n",
    "zs = []\n",
    "xs = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "\n",
    "    # randomly draw right and left rates\n",
    "    rate_r = np.random.randint(0,total_rate+1)\n",
    "    rate_l = total_rate - rate_r\n",
    "\n",
    "    # generate binned right and left clicks\n",
    "    u_r, u_l = generate_clicks(T=trial_time,dt=dt,rate_r=rate_r,rate_l=rate_l)\n",
    "    \n",
    "    # input is sum of u_r and u_l\n",
    "    u = 0.1*u_r - 0.1*u_l\n",
    "    u = u[:,np.newaxis]\n",
    "    z, x = ddm.sample(T, input=u)\n",
    "    \n",
    "    us.append(u)\n",
    "    zs.append(z)\n",
    "    xs.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_test = []\n",
    "zs_test = []\n",
    "xs_test = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "\n",
    "    # randomly draw right and left rates\n",
    "    rate_r = np.random.randint(0,total_rate+1)\n",
    "    rate_l = total_rate - rate_r\n",
    "\n",
    "    # generate binned right and left clicks\n",
    "    u_r, u_l = generate_clicks(T=trial_time,dt=dt,rate_r=rate_r,rate_l=rate_l)\n",
    "    \n",
    "    # input is sum of u_r and u_l\n",
    "    u = 0.1*u_r - 0.1*u_l\n",
    "    u = u[:,np.newaxis]\n",
    "    z, x = ddm.sample(T, input=u)\n",
    "    \n",
    "    us_test.append(u)\n",
    "    zs_test.append(z)\n",
    "    xs_test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(xs, axis=0)[:, 0]\n",
    "x_std = np.std(xs, axis=0)[:, 0]\n",
    "\n",
    "plt.fill_between(np.arange(T), x_mean-2*x_std, x_mean+2*x_std, color='k', edgecolor=\"none\", alpha=0.2)\n",
    "plt.plot(x_mean, '-k', lw=3)\n",
    "for x in xs:\n",
    "    plt.plot(x, '-k', lw=.5, alpha=.25)\n",
    "\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"$x(t)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.sort(np.random.randn(100))[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent state and a bunch of linear-nonlinear projections of it\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "i = 2\n",
    "gs = GridSpec(3, 1, height_ratios=[1, 5, 5])\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.plot(us[i])\n",
    "plt.plot(np.zeros(T), ':k')\n",
    "plt.ylim(-.5, .5)\n",
    "plt.yticks([0])\n",
    "plt.ylabel(\"input\")\n",
    "plt.xlim(0, T)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.plot(xs[i])\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "plt.ylim(-1.25, 1.25)\n",
    "plt.yticks([-1, 0, 1])\n",
    "plt.ylabel(\"latent state\")\n",
    "plt.xlim(0, T)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(gs[2])\n",
    "# y = np.maximum(0, 2 + xs[i].dot(np.random.randn(1, 100)))\n",
    "y = np.log(1 + np.exp(2 + xs[i].dot(C))) + .25 * np.random.rand(100, 100)\n",
    "plt.imshow(np.maximum(y.T, 0), aspect=\"auto\", vmin=0, cmap=\"Greys\")\n",
    "# plt.plot(y)\n",
    "# plt.ylim(-1.25, 1.25)\n",
    "# plt.yticks([])\n",
    "plt.xlim(0, T)\n",
    "plt.ylabel(\"firing rate\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accumulator_1d_{}.pdf\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ddm = DDM(beta=0.0, sigmas=np.ones((3, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = test_ddm.fit(xs, inputs=us, num_em_iters=25)\n",
    "\n",
    "plt.plot(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accum = Accumulator(beta=0.0, sigmas=np.ones((3, 1)), As=np.ones((3,1,1)))\n",
    "test_accum.fit(xs, inputs=us, num_em_iters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddm.observations.beta)\n",
    "print(ddm.observations._log_sigmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ddm.observations.beta)\n",
    "print(test_ddm.observations._log_sigmasq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True: \", ddm.log_likelihood(xs_test, inputs=us_test))\n",
    "print(\"Test: \", test_ddm.log_likelihood(xs_test, inputs=us_test))\n",
    "print(\"Test: \", test_accum.log_likelihood(xs_test, inputs=us_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative parameterization of DDM with discrete input levels**\n",
    "Here we consider the DDM for random dot motion (RDM). In RDM tasks, we typically have discrete input levels, and it is useful to consider learning a parameter $V_{z_t}$ for each input level. Let us say we have $C$ coherence levels. We can then parameterize with a one-hot input encoding the coherence level, such that $u_t \\in \\mathbb{R}^C$. Then, we can learn a $C$ dimensional vector of parameters $V_{z_t}$. The dynamics model in this case would be\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_t &\\sim p(x_{t-1}) \\\\\n",
    "x_t &= x_{t-1} + V_{z_t}^\\top u_t + \\epsilon_t \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulator models\n",
    "In the next section we develop accumulator models based on Brunton et al., Science 2013. The 1D accumulator model from Brunton 2013 has the form\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_t &= \n",
    "\\begin{cases}\n",
    "0 & \\text{if } |x_{t-1}|>B \\\\\n",
    "(1 + \\lambda) \\, x_{t-1} + \\left( \\delta_{t,t_R} \\eta_R c_t - \\delta_{t,t_L} \\eta_L c_t \\right) + \\epsilon_t & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_a^2)$, $\\eta_R \\sim \\mathcal{N}(1, \\sigma_s^2)$, $\\eta_L \\sim \\mathcal{N}(1, \\sigma_s^2)$ and \n",
    "$$\n",
    "\\begin{align*}\n",
    "c_t &= \\left[ \\frac{\\tau_\\varphi - 1}{\\tau_\\varphi} + (\\varphi - 1) (\\delta_{t,t_R} + \\delta_{t,t_L}) \\right] c_{t-1} + \\frac{1}{\\tau_\\varphi}\n",
    "\\end{align*}.\n",
    "$$\n",
    "The boundary $B$ acts in the same mechanism as the boundary in the DDM. The initial value $x_0$ comes from a zero-mean normal distribution with variance $\\sigma_i^2$.\n",
    "\n",
    "We can write the dynamics in the accumulation state (before boundary) with a coupled LDS\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_0 & \\sim \\mathcal{N}(0, \\sigma_i^2) \\\\\n",
    "x_t & \\sim \\mathcal{N} \\bigg((1 + \\lambda) \\, x_{t-1} + ( \\delta_{t,t_R} -  \\delta_{t,t_L} ) c_t , \\, \\sigma_a^2 + (\\delta_{t,t_R}^2 + \\delta_{t,t_L}^2)\\,  c_t^2 \\, \\sigma_s^2 \\bigg)\\\\\n",
    "c_t & \\sim \\mathcal{N} \\bigg(\\left[ \\frac{\\tau_\\varphi - 1}{\\tau_\\varphi} + (\\varphi - 1) (\\delta_{t,t_R} + \\delta_{t,t_L}) \\right] c_{t-1} + \\frac{1}{\\tau_\\varphi}, \\, \\sigma_c^2 \\bigg)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Brunton, B. W., Botvinick, M. M., & Brody, C. D. (2013). Rats and humans can optimally accumulate evidence for decision-making. Science, 340(6128), 95-98.\n",
    "\n",
    "DePasquale et al., Cosyne 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Accumulator with decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include accumulator drift via the modified dynamics\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_t &= A_{z_t} x_{t-1} + V_{z_t} u_t + \\epsilon_t.\n",
    "\\end{align*}\n",
    "$$\n",
    "Here, $A_\\textsf{ramp}$ corresponds to $1 + \\lambda$ from the (Brunton, 2013) accumulator model. We note that in the current implementation of this model, the $A_{z_t}$ are learned for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AccumulatorObservations(DDMObservations):\n",
    "    def __init__(self, K, D, M=1, lags=1, beta=1.0, sigmas=1e-3 * np.ones((3, 1)), As=np.ones((3,1,1))):\n",
    "        super(AccumulatorObservations, self).__init__(K, D, M, beta=beta, sigmas=sigmas)\n",
    "\n",
    "        # learn diagonal autoregressive dynamics \n",
    "        self._As = As\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self._log_sigmasq, self._As\n",
    "\n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        self.beta, self._log_sigmasq, self._As = value\n",
    "        mask = np.reshape(np.array([0, 1, 0]), (3, 1, 1))\n",
    "        self.Vs = mask * self.beta\n",
    "\n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None, covariances=None):\n",
    "        pass\n",
    "\n",
    "    def m_step(self, expectations, datas, inputs, masks, tags, covariances, **kwargs):\n",
    "        _Observations.m_step(self, expectations, datas, inputs, masks, tags, covariances, **kwargs)\n",
    "\n",
    "    \n",
    "def Accumulator(beta=1.0, sigmas=np.array([[1e-5], [1e-3], [1e-5]]), As=np.ones((3, 1, 1))):\n",
    "    K, D, M = 3, 1, 1\n",
    "    \n",
    "    # Build the initial state distribution, the transitions, and the observations\n",
    "    init_state_distn = InitialStateDistribution(K, D, M)\n",
    "    init_state_distn.log_pi0 = np.log([0.01, 0.98, 0.01])\n",
    "    transition_distn = DDMTransitions(K, D, M)\n",
    "    observation_distn = AccumulatorObservations(K, D, M, beta=beta, sigmas=sigmas, As=As)\n",
    "    \n",
    "    return BaseHMM(K, D, M, init_state_distn, transition_distn, observation_distn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "As = np.array([[1.0,0.95,1.0]]).reshape((3,1,1))\n",
    "accum = Accumulator(sigmas=[[1e-5], [1e-3], [1e-5]], As=As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample state trajectories\n",
    "T = 100 # number of time bins\n",
    "trial_time = 1.0 # trial length in seconds\n",
    "dt = 0.01 # bin size in seconds\n",
    "N_samples = 100 \n",
    "\n",
    "# input statistics\n",
    "total_rate = 40 # the sum of the right and left poisson process rates is 40\n",
    "\n",
    "us = []\n",
    "zs = []\n",
    "xs = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "\n",
    "    # randomly draw right and left rates\n",
    "    rate_r = np.random.randint(0,total_rate+1)\n",
    "    rate_l = total_rate - rate_r\n",
    "\n",
    "    # generate binned right and left clicks\n",
    "    u_r, u_l = generate_clicks(T=trial_time,dt=dt,rate_r=rate_r,rate_l=rate_l)\n",
    "    \n",
    "    # input is sum of u_r and u_l\n",
    "    u = 0.1*u_r - 0.1*u_l\n",
    "    u = u[:,np.newaxis]\n",
    "    z, x = accum.sample(T, input=u)\n",
    "    \n",
    "    us.append(u)\n",
    "    zs.append(z)\n",
    "    xs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(xs, axis=0)[:, 0]\n",
    "x_std = np.std(xs, axis=0)[:, 0]\n",
    "\n",
    "plt.fill_between(np.arange(T), x_mean-2*x_std, x_mean+2*x_std, color='k', edgecolor=\"none\", alpha=0.2)\n",
    "plt.plot(x_mean, '-k', lw=3)\n",
    "for x in xs:\n",
    "    plt.plot(x, '-k', lw=.5, alpha=.25)\n",
    "\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"$x(t)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accum = Accumulator(beta=0.0, sigmas=np.ones((3, 1)), As=np.ones((3,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = test_accum.fit(xs, inputs=us, num_em_iters=25)\n",
    "\n",
    "plt.plot(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ddm = DDM(beta=0.0, sigmas=np.ones((3, 1)))\n",
    "test_ddm.fit(xs, inputs=us, num_em_iters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accum.observations.beta)\n",
    "print(accum.observations._log_sigmasq)\n",
    "print(accum.observations._As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accum.observations.beta)\n",
    "print(test_accum.observations._log_sigmasq)\n",
    "print(test_accum.observations._As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample test data \n",
    "\n",
    "T = 100 # number of time bins\n",
    "trial_time = 1.0 # trial length in seconds\n",
    "dt = 0.01 # bin size in seconds\n",
    "N_samples = 100 \n",
    "\n",
    "# input statistics\n",
    "total_rate = 40 # the sum of the right and left poisson process rates is 40\n",
    "\n",
    "us_test = []\n",
    "zs_test = []\n",
    "xs_test = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "\n",
    "    # randomly draw right and left rates\n",
    "    rate_r = np.random.randint(0,total_rate+1)\n",
    "    rate_l = total_rate - rate_r\n",
    "\n",
    "    # generate binned right and left clicks\n",
    "    u_r, u_l = generate_clicks(T=trial_time,dt=dt,rate_r=rate_r,rate_l=rate_l)\n",
    "    \n",
    "    # input is sum of u_r and u_l\n",
    "    u = 0.1*u_r - 0.1*u_l\n",
    "    u = u[:,np.newaxis]\n",
    "    z, x = accum.sample(T, input=u)\n",
    "    \n",
    "    us_test.append(u)\n",
    "    zs_test.append(z)\n",
    "    xs_test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True: \", accum.log_likelihood(xs_test, inputs=us_test))\n",
    "print(\"Test: \", test_accum.log_likelihood(xs_test, inputs=us_test))\n",
    "print(\"Test: \", test_ddm.log_likelihood(xs_test, inputs=us_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulation to bound with 2 input dimensions: let $u_t \\in \\mathbb{R}^2$ and $V_{z_t} \\in \\mathbb{R}^2$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_t &= (1 + \\lambda) x_{t-1} + V_{z_t}^\\top u_t + \\epsilon_t \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do -> the V matrix should be D by M, or you should assert that D = M. \n",
    "\n",
    "class Accumulator2DObservations(AutoRegressiveDiagonalNoiseObservations):\n",
    "    def __init__(self, K, D=2, M=0, lags=1, betas=np.ones(2,), sigmas=1e-3 * np.ones((3, 2)), a_diag=np.ones((3,2,1))):\n",
    "        assert K == 3\n",
    "        assert D == 2\n",
    "        super(Accumulator2DObservations, self).__init__(K, D, M)\n",
    "        \n",
    "        # dynamics matrix for accumulation state\n",
    "        self._a_diag = a_diag\n",
    "        mask = np.array([np.eye(D), np.eye(D), np.eye(D)])\n",
    "        self._As = self._a_diag*mask\n",
    "        \n",
    "        # old version with a_diag a 2-dimensional vector\n",
    "#         self._As[0] = np.eye(D)\n",
    "#         self._As[1] = a_diag*np.eye(D)\n",
    "#         self._As[2] = np.eye(D)\n",
    "\n",
    "        # set input accumulator params, one for each dimension\n",
    "        self._betas = betas\n",
    "        \n",
    "        # They only differ in their input \n",
    "        self.Vs[0] *= np.zeros((D,D))            # left bound\n",
    "        self.Vs[1] = self._betas*np.eye(D)       # ramp\n",
    "        self.Vs[2] *= np.zeros((D,D))            # right bound\n",
    "        \n",
    "        # set noise variances, which are initialized in the AR constructor\n",
    "        self._log_sigmasq = np.log(sigmas)\n",
    "        \n",
    "        # Set the remaining parameters to fixed values\n",
    "        self.bs = np.zeros((3, D))\n",
    "        self.mu_init = np.zeros((3, D))\n",
    "        self._log_sigmasq_init = np.log(.01 * np.ones((3, D)))\n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._betas, self._log_sigmasq, self._a_diag\n",
    "        \n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        D = self.D\n",
    "        self._betas, self._log_sigmasq, self._a_diag = value\n",
    "        mask = np.array([np.zeros((D,D)), np.eye(D), np.zeros((D,D))])\n",
    "        self.Vs = mask * self._betas\n",
    "        a_mask = np.array([np.eye(D), np.eye(D), np.eye(D)])\n",
    "#         self._As = a_mask * self._a_diag\n",
    "        self._As = self._a_diag*a_mask         # old version with a_diag a 2-dimensional vector\n",
    "        \n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None, covariances=None):\n",
    "        pass\n",
    "\n",
    "    def m_step(self, expectations, datas, inputs, masks, tags, covariances, **kwargs):\n",
    "        _Observations.m_step(self, expectations, datas, inputs, masks, tags, covariances, **kwargs)\n",
    "    \n",
    "\n",
    "# Transition model\n",
    "class Accumulator2DTransitions(RecurrentOnlyTransitions):\n",
    "    def __init__(self, K, D, M=0, scale=100):\n",
    "        assert K == 3\n",
    "        assert D == 2\n",
    "        assert M == 2\n",
    "        super(Accumulator2DTransitions, self).__init__(K, D, M)\n",
    "        \n",
    "        self.Ws = np.zeros((K,M))\n",
    "        self.Rs = np.array([[scale, -scale], [0, 0], [-scale, scale]]).reshape((K,D)) # K by D\n",
    "        self.r = np.array([-scale, 0, -scale])\n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        return ()\n",
    "    \n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        pass\n",
    "    \n",
    "    def initialize(self, datas, inputs=None, masks=None, tags=None, covariances=None):\n",
    "        pass\n",
    "    \n",
    "    def m_step(self, expectations, datas, inputs, masks, tags, covariances, **kwargs):\n",
    "        pass\n",
    "    \n",
    "def Accumulator2D(D=2, M=2, betas=np.ones(2,), sigmas=np.array([[2e-4,1e-4],[3e-4,5e-4],[1e-4,2e-4]]), a_diag=np.ones((3,2,1))):\n",
    "    K, D, M = 3, 2, 2\n",
    "    \n",
    "    # Build the initial state distribution, the transitions, and the observations\n",
    "    init_state_distn = InitialStateDistribution(K, D, M)\n",
    "    init_state_distn.log_pi0 = np.log([0.01, 0.98, 0.01])\n",
    "    transition_distn = Accumulator2DTransitions(K, D, M)\n",
    "    observation_distn = Accumulator2DObservations(K, D, M, betas=betas, sigmas=sigmas, a_diag=a_diag)\n",
    "    \n",
    "    return BaseHMM(K, D, M, init_state_distn, transition_distn, observation_distn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmas = np.array([[1e-4], [1e-3], [1e-5]])*np.ones((3,2))\n",
    "sigmas = np.array([[2e-4,1e-4],[3e-4,5e-4],[1e-4,2e-4]])\n",
    "betas = np.array([1.1,0.9])\n",
    "a_diag = np.ones((3,2,1))\n",
    "# a_diag = np.array([[1.0,1.0],[0.98,0.95],[1.0,1.0]]).reshape((3,2,1))\n",
    "acc2 = Accumulator2D(sigmas=sigmas,betas=betas,a_diag=a_diag)\n",
    "\n",
    "# Sample state trajectories\n",
    "T = 100 # number of time bins\n",
    "trial_time = 1.0 # trial length in seconds\n",
    "dt = 0.01 # bin size in seconds\n",
    "N_samples = 100 \n",
    "\n",
    "# input statistics\n",
    "total_rate = 40 # the sum of the right and left poisson process rates is 40\n",
    "\n",
    "us = []\n",
    "zs = []\n",
    "xs = []\n",
    "\n",
    "for smpl in range(N_samples):\n",
    "\n",
    "    # randomly draw right and left rates\n",
    "    rate_r = np.random.randint(0,total_rate+1)\n",
    "    rate_l = total_rate - rate_r\n",
    "\n",
    "    # generate binned right and left clicks\n",
    "    u_r, u_l = generate_clicks(T=trial_time,dt=dt,rate_r=rate_r,rate_l=rate_l)\n",
    "    \n",
    "    # input is sum of u_r and u_l\n",
    "    u = 0.1*np.array([u_r,u_l]).T\n",
    "    z, x = acc2.sample(T, input=u)\n",
    "    \n",
    "    us.append(u)\n",
    "    zs.append(z)\n",
    "    xs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 0\n",
    "# tr += 1\n",
    "plt.plot(1 - zs[tr])\n",
    "plt.plot(xs[tr][:, 0], 'b')\n",
    "plt.plot(xs[tr][:, 1], 'r')\n",
    "# plt.plot(np.ones(T), ':k')\n",
    "# plt.plot(-np.ones(T), ':k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accum = Accumulator2D(betas=np.zeros(2,), sigmas=np.ones((3, 2)), a_diag=0.9*np.ones((3,2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = test_accum.fit(xs, inputs=us, num_em_iters=25)\n",
    "\n",
    "plt.plot(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc2.observations._betas)\n",
    "print(acc2.observations._log_sigmasq)\n",
    "print(acc2.observations._a_diag)\n",
    "\n",
    "print(test_accum.observations._betas)\n",
    "print(test_accum.observations._log_sigmasq)\n",
    "print(test_accum.observations._a_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True: \", acc2.log_likelihood(xs, inputs=us))\n",
    "print(\"Test: \", test_accum.log_likelihood(xs, inputs=us))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Models**\n",
    "1. Include adaptation $c_t$\n",
    "2. Add input-dependent variance from Bing's model\n",
    "3. Time-lag in inputs - some mechanism to incorporate time-delays from stimulus presentation until neural response.\n",
    "4. Gain modulation. Add an initial dimension\n",
    "$$\n",
    "g_t \\sim \\mathcal{N}(g_{t-1}, \\sigma_g^2)\n",
    "$$\n",
    "that modulates the mapping from latent state to neural responses. This could vary slowly within trials and/or more rapidly across trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choice = np.random.choice(2, size=100)\n",
    "choice = np.array(sorted(choice))\n",
    "C = np.random.rand(100, 2) * (np.arange(2)[None, :] == choice[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent state and a bunch of linear-nonlinear projections of it\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "i = 1\n",
    "gs = GridSpec(3, 1, height_ratios=[1, 5, 5])\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.plot(us[i])\n",
    "plt.plot(np.zeros(T), ':k')\n",
    "plt.ylim(-.5, .5)\n",
    "plt.yticks([0])\n",
    "plt.ylabel(\"input\")\n",
    "plt.xlim(0, T)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "plt.plot(xs[i])\n",
    "plt.plot(np.ones(T), ':k')\n",
    "plt.plot(-np.ones(T), ':k')\n",
    "plt.ylim(-1.25, 1.25)\n",
    "plt.yticks([-1, 0, 1])\n",
    "plt.ylabel(\"latent state\")\n",
    "plt.xlim(0, T)\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(gs[2])\n",
    "# y = np.maximum(0, 2 + xs[i].dot(np.random.randn(1, 100)))\n",
    "# C = np.random.rand(100, 2) * np.random.dirichlet(.1 * np.ones(2), size=(100))\n",
    "y = np.log(1 + np.exp(2 + xs[i].dot(C.T)))\n",
    "plt.imshow(np.maximum(y.T, 0), aspect=\"auto\", vmin=0, cmap=\"Greys\")\n",
    "# plt.plot(y)\n",
    "# plt.ylim(-1.25, 1.25)\n",
    "# plt.yticks([])\n",
    "plt.xlim(0, T)\n",
    "plt.ylabel(\"firing rate\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accumulator_2d_{}.pdf\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(C, aspect=\"auto\")\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
